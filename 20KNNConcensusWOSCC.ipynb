{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCKhoqsV3l6a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# PyTorch related imports would go here\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from us import states\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WQn0OK13vUb"
   },
   "outputs": [],
   "source": [
    "SETTINGS_NO = \"Case8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "s0UxJ27s3xRn",
    "outputId": "d3f63fef-a1d5-49ae-cc8b-7ba94ed9ef7b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# # This will prompt you to upload a file from your local system\n",
    "# uploaded = files.upload()\n",
    "# filename = next(iter(uploaded))  # Get the first (and only) filename\n",
    "\n",
    "# DATASET_NAME = \"SPECIATE_V01282025_73_species.csv\"\n",
    "DATASET_NAME = \"FinalDataset_transposed_02282025.csv\"\n",
    "\n",
    "df = pd.read_csv('PM25-Speciated/datasetasoffeb282025/'+DATASET_NAME, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liu4YFAv3zRo"
   },
   "outputs": [],
   "source": [
    "# #DATASET_NAME = \"FinalDataset_transposed_02282025.csv\"\n",
    "# DATASET_NAME =filename\n",
    "# df = pd.read_csv(DATASET_NAME, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT2fn4Jz4uJ7"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv2ucHz14TqK"
   },
   "outputs": [],
   "source": [
    "unique_scc1_values = df['SCC2'].unique()\n",
    "pivot_df = df\n",
    "pivot_df['SCC1'] = pd.factorize(pivot_df['SCC1'])[0]\n",
    "pivot_df['SCC3'] = pd.factorize(pivot_df['SCC3'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggeLdCPu45tz",
    "outputId": "fbc32999-c845-4853-fd1e-ddc5911e3fc1"
   },
   "outputs": [],
   "source": [
    "# Step 2: Ensure SCC2 is treated as a string column\n",
    "pivot_df['SCC2'] = pivot_df['SCC2'].astype(str)\n",
    "\n",
    "# Split SCC2 based on ;\n",
    "split_SCC2 = pivot_df['SCC2'].str.split(';', expand=True)\n",
    "\n",
    "# Get unique sentence fragments\n",
    "unique_fragments = pd.unique(split_SCC2.values.ravel('K'))\n",
    "\n",
    "print(unique_fragments)\n",
    "\n",
    "# Remove None values (if any)\n",
    "unique_fragments = [fragment for fragment in unique_fragments if pd.notna(fragment) and fragment != 'nan']\n",
    "\n",
    "# Create one-hot encoding for each fragment\n",
    "one_hot_encoded = pd.DataFrame(0, index=pivot_df.index, columns=unique_fragments)\n",
    "\n",
    "print(one_hot_encoded)\n",
    "\n",
    "for col in split_SCC2.columns:\n",
    "    for idx, fragment in split_SCC2[col].items():\n",
    "        if pd.notna(fragment) and fragment != 'nan':\n",
    "            one_hot_encoded.at[idx, fragment] = 1\n",
    "\n",
    "# Step 3: Convert each row of one-hot encoding into a tuple\n",
    "one_hot_tuples = [tuple(row) for row in one_hot_encoded.values]\n",
    "\n",
    "# Step 4: Assign a unique sequential number to each unique combination\n",
    "unique_combinations = pd.factorize(one_hot_tuples)[0]\n",
    "\n",
    "# Add the sequential numbers to the DataFrame\n",
    "pivot_df['SCC2'] = unique_combinations\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "YDIjYlty4_EV",
    "outputId": "5a9d004d-0ebc-4bca-cb4c-78bd29288f5c"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "\n",
    "print(pivot_df.columns[3:])\n",
    "\n",
    "# imputer = IterativeImputer()\n",
    "#imputer = SimpleImputer(strategy='median')  #  option 1\n",
    "imputer = KNNImputer() # option 2\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(pivot_df.iloc[:,3:]), columns=df.columns[3:])\n",
    "pivot_df[pivot_df.columns[3:]]=df_imputed\n",
    "\n",
    "# # Opton 3 for minimum\n",
    "# min_values = pivot_df.iloc[:, 3:].min()\n",
    "# imputers = {col: SimpleImputer(strategy='constant', fill_value=min_val) for col, min_val in min_values.items()}\n",
    "# # Apply imputation column by column\n",
    "# for col in pivot_df.columns[3:]:\n",
    "#     pivot_df[[col]] = imputers[col].fit_transform(pivot_df[[col]])\n",
    "\n",
    "pivot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6flXZ3S5EBr"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1UKuyHG5M2G"
   },
   "outputs": [],
   "source": [
    "def assign_cluster_name(cluster):\n",
    "    names = pivot_df[pivot_df['speciated_cluster'] == cluster]['PROFILE_NAME']\n",
    "    most_common_name = Counter(names).most_common(1)[0][0]\n",
    "    return most_common_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey8LD8T-72N1"
   },
   "outputs": [],
   "source": [
    "com_map = {}\n",
    "loop_info = []\n",
    "#parameters\n",
    "StartCol=3\n",
    "k_SVD = 16\n",
    "K_knn = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-_zeeR45NYA"
   },
   "outputs": [],
   "source": [
    "#X = pivot_df.iloc[:, 3:]\n",
    "## scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1O_LwUrOCJI"
   },
   "outputs": [],
   "source": [
    "#pip install cdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1pR5H7OEU4-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import cdlib\n",
    "from cdlib import algorithms\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "rLaCTTWrrkTn",
    "outputId": "b87062c7-1024-4304-d2c4-080aca716414",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pivot_df.iloc[:, 6:]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustertype = \"ClusterswoSCC\"\n",
    "folder = \"PM25-Speciated/\"+clustertype+\"/\"\n",
    "# folder = \"PM25-Speciated/ClusterswSCC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeuaKjLoFuMg",
    "outputId": "15be59bf-4878-4f0d-9cdd-00635b10145d"
   },
   "outputs": [],
   "source": [
    "# Assuming df and pivot_df are your dataframes loaded earlier\n",
    "\n",
    "# Loop for different values of k_SVD, K_knn, and StartCol\n",
    "k_SVD_values = [10, 15, 20,30]  # Example values for k_SVD\n",
    "K_knn_values = [3, 4,5, 6,7,8,9]  # Example values for K_knn\n",
    "# change this to 3 without SCC hotcoding data\n",
    "StartCol_values = [6]#[3, 4,5, 6]  # Example start columns\n",
    "\n",
    "AllGraphs=[]\n",
    "\n",
    "for StartCol in StartCol_values:\n",
    "    for k_SVD in k_SVD_values:\n",
    "        for K_knn in K_knn_values:\n",
    "            X = pivot_df.iloc[:, StartCol:]\n",
    "\n",
    "            # Apply SVD to X\n",
    "            U, S, VT = svds(X.values, k_SVD)\n",
    "            X_svd = U @ np.diag(S) @ VT\n",
    "            X = X_svd\n",
    "\n",
    "            # Compute KNN\n",
    "            knn = NearestNeighbors(n_neighbors=K_knn)  # You can change the number of neighbors\n",
    "            knn.fit(X)\n",
    "            distances, indices = knn.kneighbors(X)\n",
    "\n",
    "            # Create a graph\n",
    "            G = nx.Graph()\n",
    "\n",
    "            # Add nodes with PROFILE_NAME as labels\n",
    "            for i, profile_name in enumerate(df['PROFILE_NAME']):\n",
    "                G.add_node(i, label=profile_name)\n",
    "\n",
    "            # Add edges based on KNN\n",
    "            for i, neighbors in enumerate(indices):\n",
    "                for neighbor in neighbors:\n",
    "                    if i != neighbor:  # Avoid self-loops\n",
    "                        G.add_edge(i, neighbor, weight=1.0)\n",
    "                        \n",
    "            \n",
    "            AllGraphs.append(G)\n",
    "            \n",
    "\n",
    "#             # Apply Louvain algorithm\n",
    "#             coms = algorithms.louvain(G, weight='weight', resolution=1., randomize=False)\n",
    "#             #print(coms)\n",
    "\n",
    "#             for i in range(len(coms.communities)):\n",
    "#                 community = coms.communities[i]\n",
    "#                 for j in community:\n",
    "#                     if j in com_map:\n",
    "#                         com_map[j].append(i)\n",
    "#                     else:\n",
    "#                         com_map[j]=[i]\n",
    "#             cluster_number = len(coms.communities)\n",
    "#             #print(cluster_number)\n",
    "#             print(f\"Current StartCol: {StartCol}, Current k_SVD: {k_SVD}, Current K_knn: {K_knn}, No. of Clusters: {cluster_number} \")\n",
    "#             print_string =f\"Current StartCol: {StartCol}, Current k_SVD: {k_SVD}, Current K_knn: {K_knn}, No. of Clusters: {cluster_number} \"\n",
    "#             loop_info.append(print_string)\n",
    "\n",
    "            print(f\"Current StartCol: {StartCol}, Current k_SVD: {k_SVD}, Current K_knn: {K_knn}\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AllGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.utils import open_file\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_graphs(AllGraphs):\n",
    "    \"\"\"\n",
    "    Combine multiple graphs using nx.compose_all() and sum edge weights.\n",
    "    \n",
    "    Args:\n",
    "        AllGraphs: List of networkx graphs with the same nodes\n",
    "        \n",
    "    Returns:\n",
    "        A new graph with combined edge weights\n",
    "    \"\"\"\n",
    "    if not AllGraphs:\n",
    "        return nx.Graph()\n",
    "    \n",
    "    # Use compose_all to combine graphs\n",
    "    combined = nx.compose_all(AllGraphs)\n",
    "    \n",
    "    # For edges that exist in multiple graphs, sum their weights\n",
    "    for u, v, data in combined.edges(data=True):\n",
    "        data['weight'] = sum(G.get_edge_data(u, v, {}).get('weight', 1) \n",
    "                          for G in AllGraphs if G.has_edge(u, v))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined_graph = combine_graphs(AllGraphs)\n",
    "\n",
    "combined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_edges_for_node_sorted(graph, node):\n",
    "    if node not in graph:\n",
    "        print(f\"Node {node} not found in the graph\")\n",
    "        return\n",
    "    \n",
    "    edges = [(neighbor, data.get('weight', 1)) for neighbor, data in graph[node].items()]\n",
    "    edges.sort(key=lambda x: x[1], reverse=True)  # Sort by weight descending\n",
    "    \n",
    "    print(f\"Edges and weights for node {node} (sorted by weight):\")\n",
    "    for neighbor, weight in edges:\n",
    "        print(f\"({node}, {neighbor}): weight = {weight}\")\n",
    "\n",
    "# Example usage\n",
    "print_edges_for_node_sorted(combined_graph, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in AllGraphs:\n",
    "    print(\"*\"*100)\n",
    "    print_edges_for_node_sorted(g,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@open_file(1, mode='wb')\n",
    "def save_mm(graph, path):\n",
    "    \"\"\"\n",
    "    Save graph in Matrix Market format with edge weights.\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph to save\n",
    "        path: File path to save to\n",
    "    \"\"\"\n",
    "    # Get the number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    \n",
    "    # Create a mapping from nodes to integer indices\n",
    "    node_index = {node: i+1 for i, node in enumerate(graph.nodes())}  # Matrix Market uses 1-based indexing\n",
    "    \n",
    "    # Write the header\n",
    "    header = f\"%%MatrixMarket matrix coordinate real symmetric\\n%\\n{num_nodes} {num_nodes} {num_edges}\\n\"\n",
    "    path.write(header.encode('ascii'))\n",
    "    \n",
    "    # Write the edges\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        weight = data.get('weight', 1)\n",
    "        path.write(f\"{node_index[u]} {node_index[v]} {weight}\\n\".encode('ascii'))\n",
    "\n",
    "# Example usage:\n",
    "# Assuming AllGraphs is your list of graphs\n",
    "save_mm(combined_graph, folder+\"concensus-\"+clustertype+\".mtx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = folder+\"concensus-\"+clustertype+\".gexf\"\n",
    "print(\"Writing gephi....\"+filename)\n",
    "nx.write_gexf(combined_graph, filename)\n",
    "print(\"Done....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Louvain algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileno = 0\n",
    "\n",
    "for w in [True, False]:\n",
    "    \n",
    "    print(\"weighted\" if w else \"unweighted\")\n",
    "\n",
    "    for resolution in [0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 2.5, 3.0]: #default 1.0\n",
    "        \n",
    "        if w:\n",
    "            coms = algorithms.louvain(combined_graph, weight='weight', resolution=resolution, randomize=False)\n",
    "        else:\n",
    "            coms = algorithms.louvain(combined_graph, resolution=resolution, randomize=False)\n",
    "        #print(coms)\n",
    "        \n",
    "        # File path where you want to save the clusters\n",
    "        output_file = folder+\"cluster_no_\"+str(fileno)+\".txt\"\n",
    "\n",
    "        # Write clusters to the file\n",
    "        with open(output_file, 'w') as f:\n",
    "            for cluster in coms.communities:\n",
    "                # Convert each node number to string and join with spaces\n",
    "                line = ' '.join(map(str, cluster))\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        print(f\"Clusters written to {output_file}\")\n",
    "\n",
    "#         for i in range(len(coms.communities)):\n",
    "#             community = coms.communities[i]\n",
    "#             for j in community:\n",
    "#                 if j in com_map:\n",
    "#                     com_map[j].append(i)\n",
    "#                 else:\n",
    "#                     com_map[j]=[i]\n",
    "        cluster_number = len(coms.communities)\n",
    "        print(f\"No. of Clusters: {cluster_number} \")\n",
    "        fileno+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuGikj3sMVqY"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# # Writing com_map to CSV\n",
    "# with open('com_map_KNNImputer_wSCC.csv', mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Node', 'Communities'])\n",
    "#     for key, value in com_map.items():\n",
    "#         writer.writerow([key, ', '.join(map(str, value))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHScUNd8RQmu"
   },
   "outputs": [],
   "source": [
    "# with open('loop_info_KNNImputer_wSCC.txt', mode='w') as file:\n",
    "#     for line in loop_info:\n",
    "#         file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nps2jacjG0Ny",
    "outputId": "a310f66c-bcc4-448c-d6fb-8351e575057c"
   },
   "outputs": [],
   "source": [
    "# com_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concensus clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# PyTorch related imports would go here\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from us import states\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustertype=\"ClusterwoSCC\"\n",
    "cluster_filename = \"PM25-Speciated/reconcensusclustering/\"+clustertype+\".pamcon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_cluster_dict(cluster_filename):\n",
    "    node_cluster = {}\n",
    "    \n",
    "    with open(cluster_filename, 'r') as file:\n",
    "        for cluster_num, line in enumerate(file, start=1):\n",
    "            # Split the line into individual node numbers\n",
    "            nodes = line.strip().split()\n",
    "            \n",
    "            for node in nodes:\n",
    "                # Convert node to integer if needed (remove int() if nodes are strings)\n",
    "                node_cluster[int(node)] = cluster_num\n",
    "                \n",
    "    return node_cluster\n",
    "\n",
    "# Example usage:\n",
    "node_cluster_dict = create_node_cluster_dict(cluster_filename)\n",
    "print(node_cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivot_df\n",
    "df['Cluster'] = df['Unnamed: 0'].map(node_cluster_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build the graph\n",
    "G = nx.Graph()\n",
    "for _, row in df.iterrows():\n",
    "#     G.add_node(row['Unnamed: 0'], cluster=row['Cluster'])\n",
    "    G.add_node(row['Unnamed: 0'], cluster=row['Cluster'], name=row['PROFILE_NAME'])\n",
    "\n",
    "for cluster in df['Cluster'].unique():\n",
    "    cluster_nodes = df[df['Cluster'] == cluster]['Unnamed: 0'].values\n",
    "    cluster_data = df[df['Cluster'] == cluster].select_dtypes(include=[np.number]).values\n",
    "    \n",
    "    if len(cluster_nodes) > 1:\n",
    "        n_neighbors = min(2, len(cluster_nodes) - 1)\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(cluster_data)\n",
    "        distances, indices = nbrs.kneighbors(cluster_data)\n",
    "        \n",
    "        for i in range(len(cluster_nodes)):\n",
    "            for j_idx in range(n_neighbors):\n",
    "                j = indices[i][j_idx]\n",
    "                if i != j:\n",
    "                    G.add_edge(cluster_nodes[i], cluster_nodes[j], weight=1.0/(distances[i][j_idx] + 1e-6))\n",
    "\n",
    "# 3. Ensure full connectivity within clusters (optional)\n",
    "for cluster in df['Cluster'].unique():\n",
    "    cluster_nodes = list(df[df['Cluster'] == cluster]['Unnamed: 0'].values)\n",
    "    if len(cluster_nodes) > 1 and not nx.is_connected(G.subgraph(cluster_nodes)):\n",
    "        for i in range(len(cluster_nodes)):\n",
    "            for j in range(i + 1, len(cluster_nodes)):\n",
    "                if not G.has_edge(cluster_nodes[i], cluster_nodes[j]):\n",
    "                    G.add_edge(cluster_nodes[i], cluster_nodes[j], weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = [i+'-'+str(j) for i,j in zip(df['PROFILE_NAME'],df['PROFILE_CODE'])]\n",
    "# y = [str(i) for i in df['PROFILE_CODE']]\n",
    "labels = dict(zip(range(len(y)), y))\n",
    "\n",
    "nx.set_node_attributes(G, labels, 'labels')\n",
    "\n",
    "# z = [str(i) for i in df['Cluster']]\n",
    "# cluster = dict(zip(range(len(z)), z))\n",
    "# nx.set_node_attributes(G, cluster, 'Cluster')\n",
    "\n",
    "filename = \"PM25-Speciated/\"+\"new_concensus-\"+clustertype+'.gexf'\n",
    "print(\"Writing gephi....\"+filename)\n",
    "nx.write_gexf(G, filename)\n",
    "print(\"Done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# view cluster (without SCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get unique cluster numbers\n",
    "unique_clusters = df['Cluster'].unique()\n",
    "\n",
    "for cluster_num in sorted(unique_clusters):\n",
    "    # Get rows belonging to this cluster\n",
    "    cluster_rows = df[df['Cluster'] == cluster_num]\n",
    "    \n",
    "    # Skip if cluster is empty (or handle NaN if present)\n",
    "    if len(cluster_rows) == 0:\n",
    "        continue\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    \n",
    "    # Select columns for boxplot (adjust 2:45 as needed)\n",
    "    boxplot_data = cluster_rows.iloc[:, 6:]\n",
    "    boxplot_data.boxplot(ax=ax, grid=False)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.title(f\"Boxplot for Cluster {cluster_num}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Create table area\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_table = divider.append_axes(\"right\", size=\"40%\", pad=0.0)\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Create table - adjust 'PROFILE_NAME' to your actual column name\n",
    "    table_data = cluster_rows[['PROFILE_NAME']]\n",
    "    table = ax_table.table(\n",
    "        cellText=table_data.values,\n",
    "        colLabels=table_data.columns,\n",
    "        cellLoc='left',\n",
    "        loc='right'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "#     table.scale(2, 2)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metal and non-metal elements\n",
    "metal_elements = [\n",
    "    'Aluminum', 'Antimony', 'Arsenic', 'Barium', 'Cadmium', 'Calcium',\n",
    "    'Chromium', 'Cobalt', 'Copper', 'Indium', 'Iron', 'Lead', 'Magnesium',\n",
    "    'Manganese', 'Mercury', 'Molybdenum', 'Nickel', 'Silver', 'Sodium',\n",
    "    'Strontium', 'Tin', 'Titanium', 'Vanadium', 'Zinc', 'Zirconium'\n",
    "]\n",
    "\n",
    "non_metal_elements = [\n",
    "    'Ammonium', 'Chloride ion', 'Elemental Carbon', 'Nitrate', 'Organic carbon',\n",
    "    'Phosphorus', 'Potassium', 'Potassium ion', 'Selenium', 'Silicon',\n",
    "    'Sodium ion', 'Sulfate', 'Sulfur'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def plot_boxplot(df, cluster_num, element_list, title_suffix):\n",
    "    \"\"\"Plots a boxplot for a given cluster and element list.\"\"\"\n",
    "    cluster_rows = df[df['Cluster'] == cluster_num]\n",
    "    \n",
    "    # Skip if cluster is empty\n",
    "    if len(cluster_rows) == 0:\n",
    "        return\n",
    "    \n",
    "    # Select only the relevant columns\n",
    "    boxplot_data = cluster_rows[element_list]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    boxplot_data.boxplot(ax=ax, grid=False)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.title(f\"Boxplot for Cluster {cluster_num} ({title_suffix})\")\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Create table area\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_table = divider.append_axes(\"right\", size=\"40%\", pad=0.0)\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Display profile names for reference\n",
    "    table_data = cluster_rows[['PROFILE_NAME']]\n",
    "    table = ax_table.table(\n",
    "        cellText=table_data.values,\n",
    "        colLabels=table_data.columns,\n",
    "        cellLoc='left',\n",
    "        loc='right'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Get unique cluster numbers\n",
    "unique_clusters = df['Cluster'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_num in sorted(unique_clusters):\n",
    "    plot_boxplot(df, cluster_num, metal_elements, \"Metals\")\n",
    "#     plot_boxplot(df, cluster_num, non_metal_elements, \"Non-Metals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_num in sorted(unique_clusters):\n",
    "#     plot_boxplot(df, cluster_num, metal_elements, \"Metals\")\n",
    "    plot_boxplot(df, cluster_num, non_metal_elements, \"Non-Metals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top K important species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top important species to display\n",
    "k_species = 5\n",
    "\n",
    "# Get the unique cluster numbers\n",
    "unique_clusters = df['Cluster'].unique()\n",
    "\n",
    "for cluster_num in sorted(unique_clusters):\n",
    "    # Get rows belonging to this cluster\n",
    "    cluster_rows = df[df['Cluster'] == cluster_num]\n",
    "\n",
    "    # Skip if no rows found (or handle NaN clusters)\n",
    "    if cluster_rows.empty:\n",
    "        continue\n",
    "\n",
    "    # Select species-related columns; adjust the column indices as needed.\n",
    "    # Here, we assume the species data is stored in all columns starting from index 6.\n",
    "    species_data = cluster_rows.iloc[:, 6:]\n",
    "\n",
    "    # Compute SVD on the species data matrix (ensure the data is numeric)\n",
    "    U, S, Vt = np.linalg.svd(species_data, full_matrices=False)\n",
    "\n",
    "    # Calculate column importance using the SVD components.\n",
    "    # We square Vt, weight by the singular values and sum along the rows to aggregate importance.\n",
    "    column_importance = np.sum(np.matmul(np.diag(S), Vt**2), axis=0)\n",
    "\n",
    "    # Identify the indices of the top k important species\n",
    "    important_columns_indices = np.argsort(-column_importance)[:k_species]\n",
    "\n",
    "    # Get the corresponding column names\n",
    "    important_columns = species_data.columns[important_columns_indices]\n",
    "\n",
    "    print(f\"Important species for Cluster {cluster_num}: {list(important_columns)}\")\n",
    "\n",
    "    # Create a figure and axes for the boxplot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    # Create the boxplot using only the key species\n",
    "    ax.boxplot(cluster_rows[important_columns])\n",
    "    ax.grid(False)\n",
    "    ax.set_xticklabels(important_columns, rotation=90)\n",
    "    plt.title(f\"Boxplot for key species in Cluster {cluster_num}\")\n",
    "\n",
    "    # Create a divider for the existing axes instance to add a table on the right\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_table = divider.append_axes(\"right\", size=\"20%\", pad=0.0)\n",
    "    ax_table.axis(\"off\")\n",
    "\n",
    "    # Create the table using the PROFILE_NAME column (adjust the column name if needed)\n",
    "    table_data = cluster_rows[['PROFILE_NAME']]\n",
    "    table = ax_table.table(\n",
    "        cellText=table_data.values,\n",
    "        colLabels=table_data.columns,\n",
    "        cellLoc='left',\n",
    "        loc='right'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "#     table.scale(2, 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
